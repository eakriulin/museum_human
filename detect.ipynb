{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection and Room Positioning Algorithm\n",
    "\n",
    "The goal of the detection process is to determine where in the room museum visitors are located. The algorithm can be described as follows:\n",
    "\n",
    "1. Each video frame is put through the YOLO neural network, which detects human bodies and heads, providing the corresponding bounding boxes.\n",
    "\n",
    "2. If the full human body is visible, the pixel at the center of the bottom edge of the body bounding box is used as the pixel corresponding to the point on the floor.\n",
    "\n",
    "3. If the full human body is not visible and only the head is detected, the body height is estimated using a head-to-body ratio derived from the training data. According to the [MRI-based anatomical model of the human head](https://pmc.ncbi.nlm.nih.gov/articles/PMC2828153/#:~:text=The%20general%20shape%20of%20the,ethnicity%2C%20sex%2C%20and%20age.), the head size is generally proportional to the body height. Using this principle, a custom coefficient specific to our dataset has been calculated. The estimated height is then projected downward from the center of the top edge of the head bounding box to determine the point on the floor, representing the approximate full-body height.\n",
    "\n",
    "4. To match body and head bounding boxes, the percentage of intersection between the two is calculated and used as a matching criterion.\n",
    "\n",
    "5. The homography matrix is used to transform the resultant pixel coordinates into the actual positions of people in the room.\n",
    "\n",
    "This notebook is designed to run locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the library with YOLO models\n",
    "%pip install ultralytics\n",
    "\n",
    "# Install the computer vision library\n",
    "%pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homography\n",
    "\n",
    "To determine the positions of people within the museum, a homography matrix was calculated. Access to the museum map was obtained, and ten specific points on the map were cross-referenced with their corresponding points in the images captured by the cameras. Using this data, a homography matrix was calculated, enabling the transformation of pixel coordinates from the camera images into real-world positions within the museum.\n",
    "\n",
    "Initially, the last (10th) points were excluded from calculations and used for validation. The predicted position on the map was `1110x603`, while the actual position was estimated to be `1128x610`. This demonstrated a good level of accuracy, especially considering the imposed limitations.\n",
    "\n",
    "<img alt=\"Detection Example\" src=\"./data/images/map.png\" width=\"500\" hspace=\"0\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted map coordinate: 1118x605\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Video coordinates\n",
    "video_coords = np.array([\n",
    "    [44, 549],\n",
    "    [256, 540],\n",
    "    [615, 398],\n",
    "    [823, 311],\n",
    "    [851, 300],\n",
    "    [1064, 327],\n",
    "    [862, 523],\n",
    "    [486, 445],\n",
    "    [750, 339],\n",
    "    [1005, 385],\n",
    "], dtype=np.float32)\n",
    "\n",
    "# Map coordinates\n",
    "map_coords = np.array([\n",
    "    [630, 308],\n",
    "    [708, 389],\n",
    "    [980, 389],\n",
    "    [1250, 388],\n",
    "    [1294, 390],\n",
    "    [1294, 602],\n",
    "    [862, 610],\n",
    "    [858, 390],\n",
    "    [1131, 388],\n",
    "    [1128, 610],\n",
    "], dtype=np.float32)\n",
    "\n",
    "# Compute the homography matrix H\n",
    "H, _ = cv2.findHomography(video_coords, map_coords)\n",
    "\n",
    "# Validate the resultant matrix\n",
    "val_video_coord = np.array([[1005, 385]], dtype=np.float32)\n",
    "val_video_coord = np.array([val_video_coord])\n",
    "\n",
    "predicted_map_coord = cv2.perspectiveTransform(val_video_coord, H)\n",
    "x, y = predicted_map_coord[0][0]\n",
    "print(f'Predicted map coordinate: {round(x)}x{round(y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection\n",
    "\n",
    "The following code brings all components together. Human bodies and heads are detected using the YOLO model. Height lines are drawn based on full-body bounding box predictions and estimated body heights derived from the head-to-body coefficient. Pixel coordinates of points on the floor are calculated and transformed into actual room positions using the homography matrix. Finally, the video outputs are generated, showcasing the results.\n",
    "\n",
    "---\n",
    "\n",
    "Example 1: Detections and Estimated Positions  \n",
    "<img src=\"./data/images/detection_example_1.png\" width=\"500\" hspace=\"0\"/>\n",
    "<img src=\"./data/images/position_example_1.png\" width=\"480\" hspace=\"0\"/>\n",
    "\n",
    "Example 2: Detections and Estimated Positions  \n",
    "<img src=\"./data/images/detection_example_2.png\" width=\"500\" hspace=\"0\"/>\n",
    "<img src=\"./data/images/position_example_2.png\" width=\"480\" hspace=\"0\"/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "HEAD_CLASS_ID = 0\n",
    "HEAD_TO_BODY_COEF = 5.9437 # Refer to head_to_body_coef.ipynb\n",
    "MAP_IMAGE_PATH = './data/images/map.png'\n",
    "\n",
    "# Load the trained YOLO model\n",
    "model = YOLO('./data/best.pt')\n",
    "\n",
    "# Replace with the desired paths\n",
    "input_video_path = '/Users/eakriulin/Downloads/museum_human_example.mp4'\n",
    "detection_output_path = '/Users/eakriulin/Downloads/mh_with_map/detection_2.mp4'\n",
    "position_output_path = '/Users/eakriulin/Downloads/mh_with_map/position_2.mp4'\n",
    "\n",
    "def detect():\n",
    "    print('Starting...')\n",
    "\n",
    "    # Capture the input video and its properties\n",
    "    video = cv2.VideoCapture(input_video_path)\n",
    "    frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(video.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    # Read properties of the map image\n",
    "    map_image = cv2.imread(MAP_IMAGE_PATH)\n",
    "    map_height, map_width, _ = map_image.shape\n",
    "\n",
    "    # Create VideoWriter objects to save the output videos\n",
    "    detection_out = cv2.VideoWriter(detection_output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "    position_out = cv2.VideoWriter(position_output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (map_width, map_height))\n",
    "\n",
    "    # Process each frame\n",
    "    current_frame_count = 0\n",
    "    print_after_each_ith_frame = int(frame_count / 100)\n",
    "    while video.isOpened():\n",
    "        has_frame, detection_frame = video.read()\n",
    "        if not has_frame:\n",
    "            break\n",
    "        \n",
    "        # Create a new map frame to output positions\n",
    "        position_frame = cv2.imread(MAP_IMAGE_PATH)\n",
    "\n",
    "        # Print the processing percentages\n",
    "        current_frame_count += 1\n",
    "        if current_frame_count % print_after_each_ith_frame == 0:\n",
    "            print(f'Processed {((current_frame_count / frame_count) * 100):.0f}%')\n",
    "\n",
    "        # Run object detection with the YOLO model\n",
    "        results = model.predict(detection_frame, conf=0.7, save=False, verbose=False)\n",
    "\n",
    "        heads = []\n",
    "        bodies = []\n",
    "        pixel_coords = []\n",
    "\n",
    "        # Loop through each detected object in the frame\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                x1, y1, x2, y2 = box.xyxy[0].numpy()\n",
    "                class_id = int(box.cls[0].numpy())\n",
    "\n",
    "                if class_id == HEAD_CLASS_ID:\n",
    "                    heads.append((x1, y1, x2, y2))\n",
    "                else:\n",
    "                    bodies.append((x1, y1, x2, y2))\n",
    "\n",
    "        matched_head_indices = set()\n",
    "\n",
    "        # Loop through each detected body, identify a matching head, draw the height line\n",
    "        for body in bodies:\n",
    "            x1, y1, x2, y2 = body\n",
    "\n",
    "            upper_body = [x1, y1, x2, y1 + (y2 - y1) * 0.2]  # Top 20% of the body bounding box\n",
    "            matched_head_idx = -1\n",
    "            matched_fraction_of_intersection = float('-inf')\n",
    "\n",
    "            # Loop through each detected head, identify the best matching one\n",
    "            for idx, head in enumerate(heads):\n",
    "                fraction_of_intersection = get_fraction_of_intersection(upper_body, head)\n",
    "                if fraction_of_intersection > matched_fraction_of_intersection:\n",
    "                    matched_head_idx = idx\n",
    "                    matched_fraction_of_intersection = fraction_of_intersection\n",
    "\n",
    "            # Fix the best matching head\n",
    "            if matched_head_idx != -1:\n",
    "                matched_head_indices.add(matched_head_idx)\n",
    "            \n",
    "            height = y2 - y1\n",
    "            middle_x = int((x2 + x1) / 2)\n",
    "            start_y = int(y1)\n",
    "            end_y = int(y1 + height)\n",
    "\n",
    "            # Store pixel coords corresponding to the position in the room\n",
    "            pixel_coords.append([middle_x, end_y])\n",
    "\n",
    "            # Draw the height line (green)\n",
    "            cv2.line(detection_frame, (middle_x, start_y), (middle_x, end_y), (100, 255, 110), 2)\n",
    "\n",
    "        # Loop through each detected head, skip the matched once, draw the estimated height line\n",
    "        for idx, head in enumerate(heads):\n",
    "            if idx in matched_head_indices:\n",
    "                continue\n",
    "\n",
    "            x1, y1, x2, y2 = head\n",
    "\n",
    "            estimated_height = (y2 - y1) * HEAD_TO_BODY_COEF\n",
    "            middle_x = int((x2 + x1) / 2)\n",
    "            start_y = int(y1)\n",
    "            end_y = int(y1 + estimated_height)\n",
    "\n",
    "            # Store pixel coords\n",
    "            pixel_coords.append([middle_x, end_y])\n",
    "\n",
    "            # Draw the estimated height line (yellow)\n",
    "            cv2.line(detection_frame, (middle_x, start_y), (middle_x, end_y), (255, 240, 100), 2)\n",
    "\n",
    "        # Convert pixel coordinates into room positions\n",
    "        for x, y in pixel_coords:\n",
    "            pixel_coord = np.array([[x, y]], dtype=np.float32)\n",
    "            pixel_coord = np.array([pixel_coord])\n",
    "\n",
    "            room_coord = cv2.perspectiveTransform(pixel_coord, H)\n",
    "            room_x = int(room_coord[0][0][0])\n",
    "            room_y = int(room_coord[0][0][1])\n",
    "\n",
    "            # Draw the circle to represent the estimated room position\n",
    "            cv2.circle(position_frame, [room_x, room_y], radius=40, color=(0, 0, 0), thickness=1)\n",
    "\n",
    "        # Write to the output videos\n",
    "        detection_out.write(detection_frame)\n",
    "        position_out.write(position_frame)\n",
    "\n",
    "    print('Finishing...')\n",
    "\n",
    "    video.release()\n",
    "    detection_out.release()\n",
    "    position_out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "        \n",
    "\n",
    "def get_fraction_of_intersection(body, head):\n",
    "    x1 = max(body[0], head[0])\n",
    "    y1 = max(body[1], head[1])\n",
    "    x2 = min(body[2], head[2])\n",
    "    y2 = min(body[3], head[3])\n",
    "\n",
    "    intersection_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    head_area = (head[2] - head[0]) * (head[3] - head[1])\n",
    "\n",
    "    return intersection_area / head_area\n",
    "\n",
    "detect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
