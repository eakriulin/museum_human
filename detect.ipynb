{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection and Room Positioning Algorithm\n",
    "\n",
    "The goal of the detection process is to determine where in the room museum visitors are located. The algorithm can be described as follows:\n",
    "\n",
    "1. Each video frame is put through the YOLO neural network, which detects human bodies and heads, providing the corresponding bounding boxes.\n",
    "\n",
    "2. If the full human body is visible, the pixel at the center of the bottom edge of the body bounding box is used as the pixel corresponding to the point on the floor.\n",
    "\n",
    "3. If the full human body is not visible and only the head is detected, the body height is estimated using a head-to-body ratio derived from the training data. According to the [MRI-based anatomical model of the human head](https://pmc.ncbi.nlm.nih.gov/articles/PMC2828153/#:~:text=The%20general%20shape%20of%20the,ethnicity%2C%20sex%2C%20and%20age.), the head size is generally proportional to the body height. Using this principle, a custom coefficient specific to our dataset has been calculated. The estimated height is then projected downward from the center of the top edge of the head bounding box to determine the point on the floor, representing the approximate full-body height.\n",
    "\n",
    "4. To match body and head bounding boxes, the percentage of intersection between the two is calculated and used as a matching criterion.\n",
    "\n",
    "5. The homography matrix is used to transform the resultant pixel coordinates into the actual positions of people in the room.\n",
    "\n",
    "This notebook is designed to run locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the library with YOLO models\n",
    "%pip install ultralytics\n",
    "\n",
    "# Install the computer vision library\n",
    "%pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homography\n",
    "\n",
    "To determine positions within the room, a homography matrix is calculated. For demonstration and experimentation purposes, and due to lacking access to the museum, a simplified example of homography matrix calculation is presented here.\n",
    "\n",
    "The measurements were taken in the living room of my house. Empty water bottles were placed across the room to serve as reference points. The `room_coords` list contains the physical positions of these bottles in the room (in centimeters), while the `pixel_coords` list contains their corresponding pixel coordinates in the image.\n",
    "\n",
    "For validation, my actual position in the room, measured as `319x403`, was compared to the predicted position derived using the homography matrix, which was `316x396`. This result shows a good accuracy in position estimation.\n",
    "\n",
    "<img alt=\"Detection Example\" src=\"./data/images/me_in_the_room.jpg\" width=\"400\" hspace=\"0\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Pixel coordinates for each bottle\n",
    "pixel_coords = np.array([\n",
    "    [413, 1872],\n",
    "    [1215, 2270],\n",
    "    [2696, 2929],\n",
    "    [1154, 1907],\n",
    "    [2218, 2216],\n",
    "    [3237, 2580],\n",
    "    [1705, 1684],\n",
    "    [2222, 1900],\n",
    "    [3243, 2023],\n",
    "    [2254, 1653],\n",
    "], dtype=np.float32)\n",
    "\n",
    "# Room positions for each bottle\n",
    "room_coords = np.array([\n",
    "    [24, 240],\n",
    "    [205, 238],\n",
    "    [386, 238],\n",
    "    [124, 298],\n",
    "    [297, 309],\n",
    "    [412, 321],\n",
    "    [135, 403],\n",
    "    [251, 380],\n",
    "    [381, 437],\n",
    "    [202, 473],\n",
    "], dtype=np.float32)\n",
    "\n",
    "# Compute the homography matrix H\n",
    "H, _ = cv2.findHomography(pixel_coords, room_coords)\n",
    "\n",
    "# Validate the resultant matrix on my position in the room\n",
    "val_pixel_coord = np.array([[2673, 1999]], dtype=np.float32)\n",
    "val_pixel_coord = np.array([val_pixel_coord])\n",
    "\n",
    "predicted_room_position = cv2.perspectiveTransform(val_pixel_coord, H)\n",
    "x, y = predicted_room_position[0][0]\n",
    "print(f'Predicted room position: {round(x)}x{round(y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection\n",
    "\n",
    "The following code brings all components together. Human bodies and heads are detected using the YOLO model. Height lines are drawn based on full-body bounding box predictions and estimated body heights derived from the head-to-body coefficient. Pixel coordinates of points on the floor are calculated and transformed into actual room positions using the homography matrix. Finally, the video outputs are generated, showcasing the results.\n",
    "\n",
    "---\n",
    "\n",
    "Detection Examples  \n",
    "<img title=\"Detection Example 1\" src=\"./data/images/detection_example_1.png\" width=\"400\" hspace=\"0\"/>\n",
    "<img title=\"Detection Example 2\" src=\"./data/images/detection_example_2.png\" width=\"400\" hspace=\"0\"/>  \n",
    "Position Examples  \n",
    "<img title=\"Position Example 1\" src=\"./data/images/position_example_1.png\" width=\"400\" hspace=\"0\"/>\n",
    "<img title=\"Position Example 2\" src=\"./data/images/position_example_2.png\" width=\"400\" hspace=\"0\"/>  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "HEAD_CLASS_ID = 0\n",
    "HEAD_TO_BODY_COEF = 5.9437 # Refer to head_to_body_coef.ipynb\n",
    "\n",
    "# Load the trained YOLO model\n",
    "model = YOLO('./data/best.pt')\n",
    "\n",
    "# Replace with the desired paths\n",
    "input_video_path = '/path/to/input/video.mp4'\n",
    "detection_output_path = '/path/to/output/detection_video.mp4'\n",
    "position_output_path = '/path/to/output/position_video.mp4'\n",
    "\n",
    "def detect():\n",
    "    print('Starting...')\n",
    "\n",
    "    # Capture the input video and its properties\n",
    "    video = cv2.VideoCapture(input_video_path)\n",
    "    frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(video.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    # Create VideoWriter objects to save the output videos\n",
    "    detection_out = cv2.VideoWriter(detection_output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "    position_out = cv2.VideoWriter(position_output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "\n",
    "    # Process each frame\n",
    "    current_frame_count = 0\n",
    "    print_after_each_ith_frame = int(frame_count / 100)\n",
    "    while video.isOpened():\n",
    "        has_frame, detection_frame = video.read()\n",
    "        if not has_frame:\n",
    "            break\n",
    "        \n",
    "        # Create a fully white frame to output room positions\n",
    "        position_frame = np.ones((frame_height, frame_width, 3), dtype=np.uint8) * 255\n",
    "\n",
    "        # Print the processing percentages\n",
    "        current_frame_count += 1\n",
    "        if current_frame_count % print_after_each_ith_frame == 0:\n",
    "            print(f'Processed {((current_frame_count / frame_count) * 100):.0f}%')\n",
    "\n",
    "        # Run object detection with the YOLO model\n",
    "        results = model.predict(detection_frame, conf=0.7, save=False, verbose=False)\n",
    "\n",
    "        heads = []\n",
    "        bodies = []\n",
    "        pixel_coords = []\n",
    "\n",
    "        # Loop through each detected object in the frame\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                x1, y1, x2, y2 = box.xyxy[0].numpy()\n",
    "                class_id = int(box.cls[0].numpy())\n",
    "\n",
    "                if class_id == HEAD_CLASS_ID:\n",
    "                    heads.append((x1, y1, x2, y2))\n",
    "                else:\n",
    "                    bodies.append((x1, y1, x2, y2))\n",
    "\n",
    "        matched_head_indices = set()\n",
    "\n",
    "        # Loop through each detected body, identify a matching head, draw the height line\n",
    "        for body in bodies:\n",
    "            x1, y1, x2, y2 = body\n",
    "\n",
    "            upper_body = [x1, y1, x2, y1 + (y2 - y1) * 0.2]  # Top 20% of the body bounding box\n",
    "            matched_head_idx = -1\n",
    "            matched_fraction_of_intersection = float('-inf')\n",
    "\n",
    "            # Loop through each detected head, identify the best matching one\n",
    "            for idx, head in enumerate(heads):\n",
    "                fraction_of_intersection = get_fraction_of_intersection(upper_body, head)\n",
    "                if fraction_of_intersection > matched_fraction_of_intersection:\n",
    "                    matched_head_idx = idx\n",
    "                    matched_fraction_of_intersection = fraction_of_intersection\n",
    "\n",
    "            # Fix the best matching head\n",
    "            if matched_head_idx != -1:\n",
    "                matched_head_indices.add(matched_head_idx)\n",
    "            \n",
    "            height = y2 - y1\n",
    "            middle_x = int((x2 + x1) / 2)\n",
    "            start_y = int(y1)\n",
    "            end_y = int(y1 + height)\n",
    "\n",
    "            # Store pixel coords corresponding to the position in the room\n",
    "            pixel_coords.append([middle_x, end_y])\n",
    "\n",
    "            # Draw the height line (green)\n",
    "            cv2.line(detection_frame, (middle_x, start_y), (middle_x, end_y), (100, 255, 110), 2)\n",
    "\n",
    "        # Loop through each detected head, skip the matched once, draw the estimated height line\n",
    "        for idx, head in enumerate(heads):\n",
    "            if idx in matched_head_indices:\n",
    "                continue\n",
    "\n",
    "            x1, y1, x2, y2 = head\n",
    "\n",
    "            estimated_height = (y2 - y1) * HEAD_TO_BODY_COEF\n",
    "            middle_x = int((x2 + x1) / 2)\n",
    "            start_y = int(y1)\n",
    "            end_y = int(y1 + estimated_height)\n",
    "\n",
    "            # Store pixel coords\n",
    "            pixel_coords.append([middle_x, end_y])\n",
    "\n",
    "            # Draw the estimated height line (yellow)\n",
    "            cv2.line(detection_frame, (middle_x, start_y), (middle_x, end_y), (255, 240, 100), 2)\n",
    "\n",
    "        # Convert pixel coordinates into room positions\n",
    "        for x, y in pixel_coords:\n",
    "            pixel_coord = np.array([[x, y]], dtype=np.float32)\n",
    "            pixel_coord = np.array([pixel_coord])\n",
    "\n",
    "            # --- Apply the real homography matrix for the museum ---\n",
    "            room_coord = [x, y] # E.g., room_coord = cv2.perspectiveTransform(pixel_coord, H)\n",
    "\n",
    "            # Draw the circle to represent the estimated room position\n",
    "            cv2.circle(position_frame, room_coord, radius=60, color=(0, 0, 0), thickness=1)\n",
    "\n",
    "        # Write to the output videos\n",
    "        detection_out.write(detection_frame)\n",
    "        position_out.write(position_frame)\n",
    "\n",
    "    print('Finishing...')\n",
    "\n",
    "    video.release()\n",
    "    detection_out.release()\n",
    "    position_out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "        \n",
    "\n",
    "def get_fraction_of_intersection(body, head):\n",
    "    x1 = max(body[0], head[0])\n",
    "    y1 = max(body[1], head[1])\n",
    "    x2 = min(body[2], head[2])\n",
    "    y2 = min(body[3], head[3])\n",
    "\n",
    "    intersection_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    head_area = (head[2] - head[0]) * (head[3] - head[1])\n",
    "\n",
    "    return intersection_area / head_area\n",
    "\n",
    "detect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
